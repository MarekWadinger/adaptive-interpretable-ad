{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import datetime as dt\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "from river import anomaly, preprocessing, utils\n",
    "from river.metrics import F1, MacroF1\n",
    "from river.metrics.base import BinaryMetric\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sys.path. insert(1, str(Path().resolve().parent))\n",
    "from functions.anomaly import ConditionalGaussianScorer\n",
    "from functions.compose import build_model, convert_to_nested_dict\n",
    "from functions.proba import MultivariateGaussian\n",
    "from functions.evaluate import print_stats, progressive_val_predict\n",
    "\n",
    "# CONSTANTS\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# FUNCTIONS\n",
    "def tune_train_model(steps, df, val_kwargs: dict = {}, **params):\n",
    "    params = convert_to_nested_dict(params)\n",
    "    model = build_model(steps, params)\n",
    "    metric: BinaryMetric = F1()\n",
    "    try:\n",
    "        val_kwargs.update(params.get(\"Val\", {}))\n",
    "        progressive_val_predict(model, df, [metric], print_every=0,  print_final=False, **val_kwargs)\n",
    "\n",
    "        return metric.get()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_random_samples(df: pd.DataFrame, num_samples=10000):\n",
    "    if len(df) <= num_samples:\n",
    "        return df\n",
    "    else:\n",
    "        return df.sample(n=num_samples, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def plot_detection(df: pd.DataFrame, y_pred):\n",
    "    df['pred'] = y_pred\n",
    "    if 'anomaly' in df.columns:\n",
    "        df = get_random_samples(df)\n",
    "        if len(df.columns) >= 4:\n",
    "            # Separate the feature columns from the target column (\"anomaly\")\n",
    "            X = df.drop(columns=['anomaly', 'pred'])\n",
    "            y = df['anomaly']\n",
    "            y_pred = df['pred']\n",
    "\n",
    "            # Apply PCA to reduce the feature columns to 2 components\n",
    "            pca = PCA(n_components=2)\n",
    "            X_pca = pca.fit_transform(X)\n",
    "\n",
    "            # Create a new DataFrame with the reduced components and \"anomaly\" column\n",
    "            df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
    "            df_pca['anomaly'] = y.values\n",
    "            df_pca['pred'] = y_pred.values\n",
    "        else:\n",
    "            print(True)\n",
    "            df_pca = pd.DataFrame(\n",
    "                df.reset_index().copy()\n",
    "                )\n",
    "            df_pca.columns = ['PC1', 'PC2', 'anomaly', 'pred']\n",
    "\n",
    "        # Plot the 2D scatter plot\n",
    "        plt.scatter(df_pca[df_pca['anomaly'] == 0]['PC1'], df_pca[df_pca['anomaly'] == 0]['PC2'])\n",
    "        plt.scatter(df_pca[df_pca['anomaly'] == 1]['PC1'], df_pca[df_pca['anomaly'] == 1]['PC2'], facecolors='none', edgecolors='r', linewidths=0.5)\n",
    "        plt.scatter(df_pca[df_pca['pred'] == 1]['PC1'], df_pca[df_pca['pred'] == 1]['PC2'], marker='x', linewidths=1)  # type: ignore\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "\n",
    "def save_results_y(df_ys, path):\n",
    "    dir_path = path\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    df_ys.to_csv(f\"{dir_path}/ys.csv\", index=False)\n",
    "\n",
    "\n",
    "def save_results_metrics(metrics_res, path):\n",
    "    dir_path = path\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    metrics_res.to_csv(f\"{dir_path}/metrics.csv\")\n",
    "\n",
    "# MODS\n",
    "class QuantileFilter(anomaly.QuantileFilter):\n",
    "  def __init__(self, anomaly_detector, q: float, protect_anomaly_detector=True):\n",
    "        super().__init__(\n",
    "            anomaly_detector=anomaly_detector,\n",
    "            protect_anomaly_detector=protect_anomaly_detector,\n",
    "            q=q\n",
    "        )\n",
    "  def predict_one(self, *args):\n",
    "    score = self.score_one(*args)\n",
    "    return self.classify(score)\n",
    "\n",
    "# SETTINGS\n",
    "\n",
    "# DETECTION ALGORITHMS\n",
    "detection_algorithms = [\n",
    "    (   \n",
    "        \"Conditional Gaussian Scorer\",\n",
    "        [[ConditionalGaussianScorer,[utils.Rolling, MultivariateGaussian]]],\n",
    "        {\n",
    "            \"ConditionalGaussianScorer__threshold\": (0.85, 0.99994),\n",
    "            \"Rolling__window_size__round\": (150 , 10000),\n",
    "            \"ConditionalGaussianScorer__t_a__int\": (50, 2000),\n",
    "            \"ConditionalGaussianScorer__grace_period__round\": (50, 1000),\n",
    "            }\n",
    "        ),\n",
    "    (   \n",
    "        \"Half-Space Trees\",\n",
    "        [preprocessing.MinMaxScaler, [QuantileFilter, anomaly.HalfSpaceTrees]],\n",
    "        {\n",
    "            \"QuantileFilter__q\": (0.85, 0.99994),\n",
    "            \"HalfSpaceTrees__n_trees__round\": (1, 20),\n",
    "            \"HalfSpaceTrees__height__round\": (2, 14),\n",
    "            \"HalfSpaceTrees__window_size__round\": (100, 400)\n",
    "            }\n",
    "        ),\n",
    "    (   \"One-Class SVM\",\n",
    "        [preprocessing.StandardScaler, [QuantileFilter, anomaly.OneClassSVM]],\n",
    "        {\n",
    "            \"QuantileFilter__q\": (0.85, 0.99994),\n",
    "            \"OneClassSVM__intercept_lr\": (0.005, 0.02),\n",
    "            }\n",
    "        ),\n",
    "    # (   \"Local Outlier Factor\",\n",
    "    #     [[QuantileFilter, anomaly.LocalOutlierFactor]],\n",
    "    #     {\n",
    "    #         \"QuantileFilter__q\": (0.85, 0.99994),\n",
    "    #         \"LocalOutlierFactor__n_neighbors__round\": (0,200),\n",
    "    #         }\n",
    "    #     ),\n",
    "]\n",
    "\n",
    "# DATASETS\n",
    "datasets = [\n",
    "    # {\n",
    "    #     \"name\": \"Load Balancing\",\n",
    "    #     \"data\": pd.read_csv(\n",
    "    #         \"data/load_balancing.csv\", index_col=0).dropna(axis=0),\n",
    "    #     \"anomaly_col\": \"anomaly\",\n",
    "    #     \"drop\": \"MPC:Request Status Code\"},\n",
    "    {\n",
    "        \"name\": \"SKAB\",\n",
    "        \"data\": pd.read_csv(\n",
    "            \"data/multivariate/alldata_skab.csv\",\n",
    "            index_col=0).dropna(axis=0),\n",
    "        \"anomaly_col\": \"is_anomaly\",\n",
    "        \"drop\": \"changepoint\"},\n",
    "    # {\n",
    "    #     \"name\": \"YAHOO\",\n",
    "    #     \"data\": pd.read_csv(\n",
    "    #         \"data/multivariate/yahoo_sub_5.csv\",\n",
    "    #         index_col=0).dropna(axis=0),\n",
    "    #     \"anomaly_col\": \"is_anomaly\",\n",
    "    #     \"drop\": None},\n",
    "    # {\n",
    "    #     \"name\": \"Room Occupancy\",\n",
    "    #     \"data\": pd.read_csv(\n",
    "    #         \"data/multivariate/Occupancy/room-occupancy-1.test.csv\",\n",
    "    #         index_col=0).dropna(axis=0),\n",
    "    #     \"anomaly_col\": \"is_anomaly\",\n",
    "    #     \"drop\": None},\n",
    "    # {\n",
    "    #     \"name\": \"Archive\",\n",
    "    #     \"data\": pd.read_csv(\n",
    "    #         \"data/multivariate/archive/TimeSeries.csv\"\n",
    "    #         ).head(80000).tail(20000),\n",
    "    #     \"anomaly_col\": pd.read_csv(\n",
    "    #         \"data/multivariate/archive/labelsTimeSeries.csv\"\n",
    "    #         ).head(80000).tail(20000)['label'],\n",
    "    #     \"drop\": None},\n",
    "]\n",
    "\n",
    "# PLOT CONFIG\n",
    "plt.figure(figsize=(len([1,1,1]) * 2 + 4, 12.5))\n",
    "plt.subplots_adjust(\n",
    "    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n",
    ")\n",
    "plot_num = 1\n",
    "\n",
    "# RUN\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for dataset in datasets:\n",
    "        # PREPROCESS DATA\n",
    "        df = dataset[\"data\"]\n",
    "        df.index = pd.to_timedelta(\n",
    "            range(0, len(df)), 'T') + pd.Timestamp.now().replace(microsecond=0)\n",
    "        if isinstance(dataset[\"anomaly_col\"], str):\n",
    "            df = df.rename(columns={dataset[\"anomaly_col\"]: \"anomaly\"})\n",
    "        elif isinstance(dataset[\"anomaly_col\"], pd.Series):\n",
    "            df_y = dataset[\"anomaly_col\"]\n",
    "            df['anomaly'] = df_y.rename('anomaly').values\n",
    "        if dataset[\"drop\"] is not None:\n",
    "            df = df.drop(columns=dataset[\"drop\"])\n",
    "        print(f\"\\n=== {dataset['name']} === [{sum(df['anomaly'])}/{len(df)}]\"\n",
    "            .ljust(80, '='))\n",
    "\n",
    "        df_ys = df[[\"anomaly\"]].copy()\n",
    "        # RUN EACH MODEL AGAINST DATASET\n",
    "        for alg in detection_algorithms:\n",
    "            print(f\"\\n===== {alg[0]}\".ljust(80, '='))\n",
    "            # INITIALIZE OPTIMIZER\n",
    "            pbounds = alg[2]\n",
    "            mod_fun = partial(tune_train_model, alg[1], df, {})\n",
    "            if len(alg) == 4:\n",
    "                kwargs = alg[3]\n",
    "            else:\n",
    "                kwargs = {}\n",
    "            # INITIALIZE METRICS\n",
    "            metrics_list = []\n",
    "\n",
    "            # TUNE HYPERPARAMETERS\n",
    "            optimizer = BayesianOptimization(\n",
    "                f=mod_fun,\n",
    "                pbounds=pbounds,\n",
    "                verbose=2,\n",
    "                random_state=RANDOM_STATE,\n",
    "                allow_duplicate_points=True\n",
    "            )\n",
    "            logger = JSONLogger(path=f\"./.results/{dataset['name']}-{alg[0]}.log\")\n",
    "            optimizer.subscribe(Events.OPTIMIZATION_END, logger)\n",
    "            optimizer.maximize(init_points=1, n_iter=5)\n",
    "            params = convert_to_nested_dict(optimizer.max[\"params\"])\n",
    "            kwargs.update(params.get(\"Val\", {}))\n",
    "            print(params)\n",
    "            model = build_model(alg[1], params)\n",
    "            # USE TUNED MODEL\n",
    "            # PROGRESSIVE PREDICT\n",
    "            y_pred, change_point, _, _ = (\n",
    "                progressive_val_predict(model, df, metrics=None, print_every=0,\n",
    "                                        **kwargs))\n",
    "            \n",
    "            # SAVE PREDICITONS\n",
    "            df_ys[f\"{alg[0]}__{params}\"] = y_pred\n",
    "        \n",
    "            #  PRINT OUT LAST DETECTION RESULTS\n",
    "            print_stats(df, y_pred, change_point)\n",
    "            plt.subplot(len(datasets), len(detection_algorithms), plot_num)\n",
    "            plot_detection(df, y_pred)\n",
    "            plot_num +=1\n",
    "        \n",
    "        # LOAD RESULTS\n",
    "        #  Save\n",
    "        dir_path = f\".results/{dataset['name']}\"\n",
    "        save_results_y(df_ys,\n",
    "                        f\".results/{dataset['name']}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from river.metrics import F1, Precision, Recall, ClassificationReport\n",
    "\n",
    "metrics = [\n",
    "    Precision(), Recall(), F1(),\n",
    "    ClassificationReport(),\n",
    "]\n",
    "\n",
    "# show folders in dir\n",
    "for folder in os.listdir(\".results\"):\n",
    "    # check if listed object is a folder and does not start with a period\n",
    "    if os.path.isdir(os.path.join(\".results\", folder)) and not folder.startswith('.'):\n",
    "        # loop through the files in the folder\n",
    "        for file in os.listdir(os.path.join(\".results\", folder)):\n",
    "            if file == \"ys.csv\":\n",
    "                df_ys = pd.read_csv(\n",
    "                    os.path.join(\".results\", folder) + \"/ys.csv\")\n",
    "                df_metrics = pd.DataFrame(index=[\n",
    "                    'Precision', 'Recall', 'F1',\n",
    "                    'MacroPrecision', 'MacroRecall', 'MacroF1',\n",
    "                    'WeightedPrecision', 'WeightedRecall', 'WeightedF1', 'FAR',\n",
    "                    'Support_False', 'Support_True'])\n",
    "                df_stats = df_metrics.copy()\n",
    "                for col in df_ys.columns[1:]:\n",
    "                    \n",
    "                    metrics_ = [metric.clone() for metric in metrics]\n",
    "                    for y_true, y_pred in zip(df_ys.anomaly, df_ys[col]):\n",
    "                        \n",
    "                        for metric in metrics_:\n",
    "                            metric = metric.update(y_true, y_pred)\n",
    "                    cr = metrics_.pop(-1)\n",
    "                    cm = cr.cm\n",
    "                    df_metrics[col] = [\n",
    "                        metric.get() for metric in metrics_] + [\n",
    "                        cr._macro_precision.get(),\n",
    "                        cr._macro_recall.get(),\n",
    "                        cr._macro_f1.get(),\n",
    "                        cr._weighted_precision.get(),\n",
    "                        cr._weighted_recall.get(),\n",
    "                        cr._weighted_f1.get(),\n",
    "                        cm.total_false_positives / (cm.total_false_positives + cm.total_true_negatives),\n",
    "                        *cm.sum_row.values()\n",
    "                        ]\n",
    "                        \n",
    "                col_pat = df_metrics.columns.str.split('__', n=2).str[0]\n",
    "                df_metrics_grouped = df_metrics.groupby(col_pat, axis=1)\n",
    "                group_idx = pd.Index(df_metrics_grouped.groups.keys())\n",
    "                df_stats[group_idx+\"_max\"] = df_metrics_grouped.max()\n",
    "                df_stats[group_idx+\"_mean\"] = df_metrics_grouped.mean()\n",
    "                df_stats[group_idx+\"_std\"] = df_metrics_grouped.std()\n",
    "            \n",
    "                dir_path = f\".results/{folder}\"\n",
    "                df_metrics.to_csv(f\"{dir_path}/metrics.csv\")\n",
    "                df_stats.to_csv(f\"{dir_path}/stats.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

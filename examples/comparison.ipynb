{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import datetime as dt\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "from river import anomaly, preprocessing, utils\n",
    "from river.metrics import F1, Precision, Recall, MacroF1, ClassificationReport\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sys.path. insert(1, str(Path().resolve().parent))\n",
    "from functions.anomaly import ConditionalGaussianScorer\n",
    "from functions.compose import build_model, convert_to_nested_dict\n",
    "from functions.proba import MultivariateGaussian\n",
    "from functions.evaluate import print_stats, progressive_val_predict\n",
    "\n",
    "# FUNCTIONS\n",
    "def tune_train_model(steps, df, val_kwargs: dict = {}, **params):\n",
    "    params = convert_to_nested_dict(params)\n",
    "    model = build_model(steps, params)\n",
    "    metrics = [MacroF1().clone()]\n",
    "    try:\n",
    "        val_kwargs.update(params.get(\"Val\", {}))\n",
    "        progressive_val_predict(model, df, metrics, print_every=0,  print_final=False, **val_kwargs)\n",
    "\n",
    "        return metrics[0].get()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_random_samples(df: pd.DataFrame, num_samples=10000):\n",
    "    if len(df) <= num_samples:\n",
    "        return df\n",
    "    else:\n",
    "        return df.sample(n=num_samples, random_state=42)\n",
    "\n",
    "\n",
    "def plot_detection(df: pd.DataFrame, y_pred):\n",
    "    df['pred'] = y_pred\n",
    "    if 'anomaly' in df.columns:\n",
    "        df = get_random_samples(df)\n",
    "        if len(df.columns) >= 4:\n",
    "            # Separate the feature columns from the target column (\"anomaly\")\n",
    "            X = df.drop(columns=['anomaly', 'pred'])\n",
    "            y = df['anomaly']\n",
    "            y_pred = df['pred']\n",
    "\n",
    "            # Apply PCA to reduce the feature columns to 2 components\n",
    "            pca = PCA(n_components=2)\n",
    "            X_pca = pca.fit_transform(X)\n",
    "\n",
    "            # Create a new DataFrame with the reduced components and \"anomaly\" column\n",
    "            df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
    "            df_pca['anomaly'] = y.values\n",
    "            df_pca['pred'] = y_pred.values\n",
    "        else:\n",
    "            print(True)\n",
    "            df_pca = pd.DataFrame(\n",
    "                df.reset_index().copy()\n",
    "                )\n",
    "            df_pca.columns = ['PC1', 'PC2', 'anomaly', 'pred']\n",
    "\n",
    "        # Plot the 2D scatter plot\n",
    "        plt.scatter(df_pca[df_pca['anomaly'] == 0]['PC1'], df_pca[df_pca['anomaly'] == 0]['PC2'])\n",
    "        plt.scatter(df_pca[df_pca['anomaly'] == 1]['PC1'], df_pca[df_pca['anomaly'] == 1]['PC2'], facecolors='none', edgecolors='r', linewidths=0.5)\n",
    "        plt.scatter(df_pca[df_pca['pred'] == 1]['PC1'], df_pca[df_pca['pred'] == 1]['PC2'], marker='x', linewidths=1)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "\n",
    "def save_results_y(df, y_pred, change_point, path):\n",
    "    df_y = pd.concat([pd.Series(df.anomaly.values), pd.Series(y_pred).astype(int), pd.Series(change_point)], axis=1)\n",
    "    df_y.columns = ['true', 'pred', 'change']\n",
    "\n",
    "    dir_path = path\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    df_y.to_csv(f\"{dir_path}/y.csv\", index=False)\n",
    "\n",
    "\n",
    "def save_results_metrics(metrics_res, path):\n",
    "    dir_path = path\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    metrics_res.to_csv(f\"{dir_path}/metrics.csv\")\n",
    "\n",
    "# MODS\n",
    "class QuantileFilter(anomaly.QuantileFilter):\n",
    "  def __init__(self, anomaly_detector, q: float, protect_anomaly_detector=True):\n",
    "        super().__init__(\n",
    "            anomaly_detector=anomaly_detector,\n",
    "            protect_anomaly_detector=protect_anomaly_detector,\n",
    "            q=q\n",
    "        )\n",
    "  def predict_one(self, *args):\n",
    "    score = self.score_one(*args)\n",
    "    return score >= (self.quantile.get() or np.inf)\n",
    "\n",
    "# SETTINGS\n",
    "\n",
    "# DETECTION ALGORITHMS\n",
    "detection_algorithms = [\n",
    "    (   \n",
    "        \"Half-Space Trees\",\n",
    "        [preprocessing.MinMaxScaler, [QuantileFilter, anomaly.HalfSpaceTrees]],\n",
    "        {\n",
    "            \"QuantileFilter__q\": (0.9, 0.99735),\n",
    "            \"HalfSpaceTrees__n_trees__round\": (5, 15)\n",
    "            }\n",
    "        ),\n",
    "    (   \n",
    "        \"Conditional Gaussian Scorer\",\n",
    "        [[ConditionalGaussianScorer,[utils.Rolling, MultivariateGaussian]]],\n",
    "        {\n",
    "            \"Rolling__window_size__round\": (150 , 500),\n",
    "            \"ConditionalGaussianScorer__grace_period__round\": (50, 1000),\n",
    "            \"ConditionalGaussianScorer__threshold\": (0.80, 0.99935),\n",
    "            \"ConditionalGaussianScorer__t_a__int\": (50, 1000)\n",
    "            }\n",
    "        ),\n",
    "    (   \"One-Class SVM\",\n",
    "        [preprocessing.StandardScaler, [QuantileFilter, anomaly.OneClassSVM]],\n",
    "        {\n",
    "            \"QuantileFilter__q\": (0.9, 0.99735)\n",
    "            }\n",
    "        ),\n",
    "]\n",
    "\n",
    "# DATASETS\n",
    "datasets = [\n",
    "    {\n",
    "        \"name\": \"Load Balancing\",\n",
    "        \"data\": pd.read_csv(\n",
    "            \"data/load_balancing.csv\", index_col=0).dropna(axis=0),\n",
    "        \"anomaly_col\": \"anomaly\",\n",
    "        \"drop\": \"MPC:Request Status Code\"},\n",
    "    {\n",
    "        \"name\": \"YAHOO\",\n",
    "        \"data\": pd.read_csv(\n",
    "            \"data/multivariate/yahoo_sub_5.csv\",\n",
    "            index_col=0).dropna(axis=0),\n",
    "        \"anomaly_col\": \"is_anomaly\",\n",
    "        \"drop\": None},\n",
    "    {\n",
    "        \"name\": \"SKAB\",\n",
    "        \"data\": pd.read_csv(\n",
    "            \"data/multivariate/alldata_skab.csv\",\n",
    "            index_col=0).dropna(axis=0),\n",
    "        \"anomaly_col\": \"is_anomaly\",\n",
    "        \"drop\": \"changepoint\"},\n",
    "    {\n",
    "        \"name\": \"Room Occupancy\",\n",
    "        \"data\": pd.read_csv(\n",
    "            \"data/multivariate/Occupancy/room-occupancy-1.test.csv\",\n",
    "            index_col=0).dropna(axis=0),\n",
    "        \"anomaly_col\": \"is_anomaly\",\n",
    "        \"drop\": None},\n",
    "    {\n",
    "        \"name\": \"Archive\",\n",
    "        \"data\": pd.read_csv(\n",
    "            \"data/multivariate/archive/TimeSeries.csv\"\n",
    "            ).head(80000).tail(20000),\n",
    "        \"anomaly_col\": pd.read_csv(\n",
    "            \"data/multivariate/archive/labelsTimeSeries.csv\"\n",
    "            ).head(80000).tail(20000)['label'],\n",
    "        \"drop\": None},\n",
    "]\n",
    "\n",
    "# METRICS\n",
    "metrics = [\n",
    "    Precision(), Recall(), F1(),\n",
    "    ClassificationReport(),\n",
    "]\n",
    "metrics_res = pd.DataFrame(columns=[\n",
    "    'Precision', 'Recall', 'F1',\n",
    "    'MacroPrecision', 'MacroRecall', 'MacroF1',\n",
    "    'WeightedPrecision', 'WeightedRecall', 'WeightedF1'])\n",
    "\n",
    "# PLOT CONFIG\n",
    "plt.figure(figsize=(len([1,1,1]) * 2 + 4, 12.5))\n",
    "plt.subplots_adjust(\n",
    "    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n",
    ")\n",
    "plot_num = 1\n",
    "\n",
    "# RUN\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for dataset in datasets:\n",
    "        # PREPROCESS DATA\n",
    "        df = dataset[\"data\"]\n",
    "        df.index = pd.to_timedelta(\n",
    "            range(0, len(df)), 'T') + dt.datetime.now().replace(microsecond=0)\n",
    "        if isinstance(dataset[\"anomaly_col\"], str):\n",
    "            df = df.rename(columns={\"is_anomaly\": \"anomaly\"})\n",
    "        elif isinstance(dataset[\"anomaly_col\"], pd.Series):\n",
    "            df_y = dataset[\"anomaly_col\"]\n",
    "            df_y.name = 'anomaly'\n",
    "            df['anomaly'] = df_y.values\n",
    "        if dataset[\"drop\"] is not None:\n",
    "            df = df.drop(columns=dataset[\"drop\"])\n",
    "        print(f\"\\n=== {dataset['name']} === [{sum(df['anomaly'])}/{len(df)}]\"\n",
    "            .ljust(80, '='))\n",
    "        # RUN EACH MODEL AGAINST DATASET\n",
    "        for alg in detection_algorithms:\n",
    "            print(f\"\\n===== {alg[0]}\".ljust(80, '='))\n",
    "            # TUNE HYPERPARAMETERS\n",
    "            pbounds = alg[2]\n",
    "            mod_fun = partial(tune_train_model, alg[1], df, {})\n",
    "            optimizer = BayesianOptimization(\n",
    "                f=mod_fun,\n",
    "                pbounds=pbounds,\n",
    "                verbose=2,\n",
    "                random_state=1,\n",
    "                allow_duplicate_points=True\n",
    "            )\n",
    "            logger = JSONLogger(path=f\"./.results/{dataset['name']}-{alg[0]}.log\")\n",
    "            optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "            optimizer.maximize(init_points=10, n_iter=10)\n",
    "            params = convert_to_nested_dict(optimizer.max[\"params\"])\n",
    "            print(params)\n",
    "            model = build_model(alg[1], params)\n",
    "            # USE TUNED MODEL\n",
    "            metrics_ = [metric.clone() for metric in metrics]\n",
    "            # PROGRESSIVE PREDICT\n",
    "            if len(alg) == 4:\n",
    "                kwargs = alg[3]\n",
    "            else:\n",
    "                kwargs = {}\n",
    "            kwargs.update(params.get(\"Val\", {}))\n",
    "            y_pred, change_point, _, _ = (\n",
    "                progressive_val_predict(model, df, metrics_, print_every=0,\n",
    "                                        **kwargs))\n",
    "            \n",
    "            # LOAD RESULTS\n",
    "            #  Save\n",
    "            dir_path = f\".results/{dataset['name']}/{alg[0]}\"\n",
    "            save_results_y(df, y_pred, change_point,\n",
    "                           f\".results/{dataset['name']}/{alg[0]}\")\n",
    "            cr = metrics_.pop(-1)\n",
    "            \n",
    "            metrics_res.loc[alg[0]] = (\n",
    "                [metric.get() for metric in metrics_] + [\n",
    "                    cr._macro_precision.get(),\n",
    "                    cr._macro_recall.get(),\n",
    "                    cr._macro_f1.get(),\n",
    "                    cr._weighted_precision.get(),\n",
    "                    cr._weighted_recall.get(),\n",
    "                    cr._weighted_f1.get()\n",
    "                    ])\n",
    "            #  Print\n",
    "            print_stats(df, y_pred, change_point)\n",
    "            plt.subplot(len(datasets), len(detection_algorithms), plot_num)\n",
    "            plot_detection(df, y_pred)\n",
    "            plot_num +=1\n",
    "        save_results_metrics(metrics_res, f\".results/{dataset['name']}\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

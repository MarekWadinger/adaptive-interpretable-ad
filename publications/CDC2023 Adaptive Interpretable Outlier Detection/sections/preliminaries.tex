In this section, we present the fundamental ideas that form the basis of the developed approach. Subsection \ref{AA:Welford} explains Welford's online algorithm, which can adjust distribution to changes in real time. Subsection \ref{AA:InvWelford} proposes a two-pass implementation that can reverse the impact of expired samples. The math behind distribution modeling in Subsection \ref{AA:Distribution} establishes the foundation for the Gaussian anomaly detection model discussed in the final Subsection \ref{AA:Anomaly} of the preliminaries.

\subsection{Welford's Online Algorithm}\label{AA:Welford}
Welford introduced a numerically stable online algorithm for calculating mean and variance in a single pass. The algorithm allows the processing of IoT device measurements without the need to store their values \cite{Wel62}.

Given measurement \(x_i\) where \(i=1,...,n\) is a sample index in sample population \(n\), the corrected sum of squares \(S_n\) is defined as
\begin{equation}
S_n = \sum_{i=1}^n (x_i - \bar x_n)^2\text{,}\label{eq:sumsquares}
\end{equation}
with the running mean \(\bar x_n\) defined as previous mean \(\bar x_{n-1}\) weighted by proportion of previously seen population \(n-1\) corrected by current sample as
\begin{equation}
\bar x_n = \frac{n-1}{n} \bar x_{n-1} + \frac{1}{n}x_n = \bar x_{n-1} + \frac{x_n - \bar x_{n-1}}{n}\text{.}\label{eq:runmean}
\end{equation}
Throughout this paper, we consider a following formulation of an update to the corrected sum of squares:
\begin{equation}
S_n = S_{n-1} + (x_n - \bar x_{n-1})(x_n - \bar x_n)\text{,}\label{eq:upsumsquares}
\end{equation}
as it is less prone to numerical instability due to catastrophic cancellation. Finally, the corresponding unbiased variance is
\begin{equation}
s^2_n = \frac{S_{n}}{n-1}\text{.}\label{eq:runvar}
\end{equation}

This implementation of the Welford method requires the storage of three scalars: \(\bar x_{n-1}\); \(n\); \(S_n\).

\subsection{Inverse Welford's Algorithm}\label{AA:InvWelford}
Based on \eqref{eq:runmean}, it is clear that the influence of the latest sample over the running mean decreases as the population \(n\) grows. For this reason, regulating the number of samples used for sample mean and variance computation has crucial importance over adaptation. Given access to the instances used for computation and expiration period \(t_e \in \mathbb{N}_{0}^{n-1}\), reverting the impact of \(x_{n-t_e}\) can be written as follows

\begin{equation}
S_{n-1} = S_n - (x_{n-t_e} - \bar x_{n-1})(x_{n-t_e} - \bar x_n)\text{,}\label{eq:revrunmean}
\end{equation}

where the reverted mean is given as

\begin{equation}
\bar x_{n-1} = \frac{n}{n-1} \bar x_{n} - \frac{1}{n-1}x_{n-t_e} = \bar x_{n} - \frac{x_{n-t_e} - \bar x_{n}}{n-1}\text{.}\label{eq:revmean}
\end{equation}


Finally, the unbiased variance follows the formula:

\begin{equation}
s^2_{n-1} = \frac{S_{n-1}}{n-2}\text{.}\label{eq:revvar}
\end{equation}

\subsection{Statistical Model of Multivariate System}\label{AA:Distribution}
Multivariate normal distribution generalizes the multivariate systems to the model where the degree to which variables are related is represented by the covariance matrix. Gaussian normal distribution of variables is a reasonable assumption for process measurements, as it is a common distribution that arises from stable physical processes measured with noise. The general notation of multivariate normal distribution is:
\begin{equation}\mathbf{X}\ \sim\ \mathcal{N}_k(\boldsymbol\mu,\, \boldsymbol\Sigma)\text{,}
\end{equation}

where $k$-dimensional mean vector is denoted as \(\boldsymbol\mu = (\bar x_{1},...,\bar x_{k})^T\ \in \mathbb{R}^{k}\) and \(\boldsymbol\Sigma \in \mathbb{R}^{k\times{k}}\) is the $k \times k$ covariance matrix, where \(k\) is the index of last random variable.

The probability density function (PDF) \(f(\boldsymbol{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma})\) of multivariate normal distribution is denoted as:
\begin{equation}
f(\boldsymbol{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{(2\pi)^{k/2} |\boldsymbol{\Sigma}|^{1/2}} e^{-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\boldsymbol{x}-\boldsymbol{\mu})}\text{,}
\end{equation}

where $\boldsymbol{x}$ is a $k$-dimensional vector of measurements $x_i$ at time $i$, $|\boldsymbol{\Sigma}|$ denotes the determinant of $\boldsymbol{\Sigma}$, and $\boldsymbol{\Sigma}^{-1}$ is the inverse of $\boldsymbol{\Sigma}$.

The cumulative distribution function (CDF) of a multivariate Gaussian distribution describes the probability that all components of the random matrix \(\boldsymbol{X}\) take on a value less than or equal to a particular point \(\boldsymbol{x}\) in space, and can be used to evaluate the likelihood of observing a particular set of measurements or data points. The CDF is often used in statistical applications to calculate confidence intervals, perform hypothesis tests, and make predictions based on observed data. In other words, it gives the probability of observing a random vector that falls within a certain region of space. The standard notation of CDF is as follows:

\begin{equation}
F(\boldsymbol{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \int_{-\infty}^{\boldsymbol{x}} f(\boldsymbol{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma})  \text{d}\boldsymbol{x}\text{,}\label{eq:cdf}
\end{equation}

where $\text{d}\boldsymbol{x}$ denotes the integration over all $k$ dimensions of $\boldsymbol{x}$.

As the equation \eqref{eq:cdf} cannot be integrated explicitly, an algorithm for numerical computation was proposed in \cite{Genz2000}.


Given the PDF, we can also determine the value of \(\boldsymbol{x}\) that corresponds to a given quantile $q$  using a numerical method for inversion of CDF (ICDF) often denoted as percent point function (PPF) or $F(\boldsymbol{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma})^{-1}$. An algorithm that calculates the value of the PPF for univariate normal distribution is reported below as Algorithm \ref{alg:ppf}.

\begin{algorithm}[H]
\caption{{Percent-Point Function for Normal Distribution}} \label{alg:ppf}
 \begin{algorithmic}[1]
 \renewcommand{\algorithmicrequire}{\textbf{Input:}}
 \renewcommand{\algorithmicensure}{\textbf{Output:}}
 \REQUIRE quantile $q$, sample mean $\bar x_n$ \eqref{eq:runmean}, sample variance $s^2_n$ \eqref{eq:runvar}
 \ENSURE  threshold value $x_{n,q}$
 \\ \textit{Initialisation} :
  \STATE $f \leftarrow 10$; $l \leftarrow -f $; $r \leftarrow f;$
 \\ \textit{LOOP Process}
  \WHILE {$F(l; \bar x_n, s^2_n) > 0$}
  \STATE $r \leftarrow l $;
  \STATE $l \leftarrow lf $;
  \ENDWHILE
  \WHILE {$F_X(r)-q < 0$}
    \STATE $l \leftarrow r $;
    \STATE $r \leftarrow rf $;
  \ENDWHILE
  \STATE {$\tilde{x}_{n,q} = \text{arg} \min_{x_n} \| F(x_n; \bar x_n, s^2_n) - q \| ~ \text{s.t.} ~ l \le x_n \le r$}
 \RETURN $\tilde{x}_{n,q}  \sqrt{s^2_n} + \bar x_n $
 \end{algorithmic}
\end{algorithm}

The Algorithm \ref{alg:ppf} for PPF computation is solved using an iterative root-finding algorithm such as Brent's method \cite{Brent72}.

\subsection{Multivariate Gaussian Anomaly Detection}\label{AA:Anomaly}
From a statistical viewpoint, outliers can be denoted
as values that significantly deviate from the mean. Assuming that the spatial and temporal characteristics of the system over the moving window can be encoded as normally distributed features, we can claim, that any anomaly may be detected as an outlier.

In empirical fields, such as machine learning, the three-sigma rule ($3\sigma$) defines a region of distribution where normal values are expected to occur with near certainty. This assumption makes approximately 0.27\% of values in the given distribution considered anomalous.

The \(3\sigma\) rule establishes the probability that any sample \(\boldsymbol{x_i}\) of a random variable \(\boldsymbol{X}\) lies within a given CDF over a semi-closed interval as the distance from the sample mean \(\boldsymbol{\mu}\) of 3 sample standard deviations \(\boldsymbol{\Sigma}\) and gives an approximate value of $q$ as
\begin{equation}
q=P\{|\boldsymbol{x_i}-\boldsymbol{\mu}|<3\boldsymbol{\Sigma}\}=0.99730\text{.}
\end{equation}

Using a probabilistic model of normal behavior lets us query the threshold vectors \(\boldsymbol{x_{l}}\) and \(\boldsymbol{x_{u}}\) which corresponds to the closed interval of CDF at which probability was established. Inversion of \eqref{eq:cdf} can be used for such query resulting in:

\begin{equation}
\boldsymbol{x_l} = F((1 - P\{|\boldsymbol{x_i}-\boldsymbol{\mu}|<3\boldsymbol{\Sigma}\circ\boldsymbol{I}\}); \boldsymbol{\mu}, \boldsymbol{\Sigma}\circ\boldsymbol{I})^{-1}\text{,}\label{eq:thresh_low}
\end{equation}

for the lower limit, and

\begin{equation}
\boldsymbol{x_u} = F((P\{|\boldsymbol{x_i}-\boldsymbol{\mu}|<3\boldsymbol{\Sigma}\circ\boldsymbol{I}\}); \boldsymbol{\mu}, \boldsymbol{\Sigma}\circ\boldsymbol{I})^{-1}\text{,}\label{eq:thresh_high}
\end{equation}

for upper one, where $\boldsymbol{\Sigma}\circ\boldsymbol{I}$ represents diagonal elements of $\boldsymbol{\Sigma}$.

However, the problem of computing CDF of a multivariate normal distribution is that it may result in numerical issues for small probabilities. To avoid underflow, the logarithm of CDF (log-CDF) is computed, converting the product of individual elements into a numerically more stable summation. The value of $T$ represents a threshold, defining the discrimination boundary between normal operation and anomaly. The predicted state of the system $Y_i$ at time $i$ is defined as
\begin{equation}
y_i =
  \begin{cases}
     0 & \text{ if } T \leq \log{F(\boldsymbol{x_i}; \boldsymbol{\mu}, \boldsymbol{\Sigma})} \\
     1 & \text{ if }  T > \log{F(\boldsymbol{x_i}; \boldsymbol{\mu}, \boldsymbol{\Sigma})}\text{,}\label{eq:anomaly}
  \end{cases}
\end{equation}

where $y_i = 0$ for normal operation of the system and $y_i = 1$ for anomalous operation.

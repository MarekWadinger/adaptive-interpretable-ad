The era of Industry 4.0 is ruled by data. Effective data-based decision-making is driven by the quantity of collected data. Internet of Things (IoT) devices made data acquisition seamless and positively influenced a wide range of industries. It is estimated that the annual economic impact of IoT will further grow and reach up to \$6.2 trillion by 2025 \cite{Manyika2013}. 

Various data collection mechanisms are used to buffer and store the data for future processing. However, the tremendous increase in data availability and the desire to extract valuable insight led to problems with the unbounded buffering and storage capacity. Real-time evaluation of the data streams became an acronym for smart data processing. 

Streaming data analytics introduced mechanisms for online extraction and transformation while loading to the storage only a fraction of the former data load, which allowed the storage of the vital information carried by the data more comprehensively. However, the unstable quality of the data appeared to have the most crucial importance over the quantity. 

Anomaly detection, well studied in the last decades, was reborn to the world of new challenges. Former studies were mainly concerned with a domain-specific detection of various anomalies while trained offline \cite{Chandola2009}. However, anomalies of diverse sources, from fraudulent web activity and suspicious financial transactions to sensor failure, malfunctioning of the hardware, and performance drops, mutate over time, and the model had to be updated.

Companies expanded their research activities on the creation and integration of generic frameworks combining prediction, detection, and alert mechanisms. One of the first projects, open-sourced for the public, are EGADS by Yahoo \cite{Laptev2015} and AnomalyDetection by Twitter \cite{Kejariwal2015}. The frameworks' modularity allowed the automation of the anomaly detection of time-series data and created space for discussion. 

Moving from domain-specific to generic methods posed new problems connected to type I errors, i.e., a false-positive classification of normal behavior as anomalous. Accurate selection of forecaster, detector, and alerting mechanism allowed to tackle the problem, nevertheless, introduced considerable dependence on expert domain knowledge and fine-tuning. 

Further work proved improvement in performance while relieving the tight requirements on domain knowledge \cite{Ahmad2016}. However, strict demands on detection systems ranging from lasting up times to continuous monitoring with stable performance pointed to the challenge of data stationarity. Change points and concept drifts troubled unsupervised models, which led to service downtime due to the model retraining.  

The era of adaptive machine learning introduced incremental learning schemes as a solution. Multiple studies for learning modes, adaptation methods, and model management swept through the machine learning community. Pannu et al. proposed an adaptive anomaly detection system \cite{Pannu2012}. However, the method represented a supervised operator-in-the-loop solution. Zhang et al. introduced an adaptive kernel density-based algorithm that uses an adaptive kernel width \cite{Zhang201850}. Nonetheless, training the models on big data had limitations resulting from the storage and unbounded buffering of data. Online learning models relaxed the need for data availability during model training \cite{Gama2014}. On the contrary, it processed the data from a bounded buffer sequentially as in \cite{Bosman201514} and \cite{Ahmad2017134}. 

Anomaly detection in microgrids, however, called for low latency detection which implied real-time training and prediction processes \cite{Liu2017}. Such adaptation of streamed modeling took into consideration strict boundaries on computational time. For work in this area see \cite{Wang2020114145} and \cite{Dai2022}.

Alerting mechanisms in process automation detect situations where signal value deviates from constraints. An alert watchdog is triggered on threshold violation by individual signals. The constraints, or process limits, are usually predefined and fixed. Nevertheless, factors such as aging and environmental changes call for dynamic process limits. Setting up a procedure for an evergrowing number of signal measurements is time-consuming. Besides, it is impossible for signals where no prior information about a correct process range is known. Those are subject to external factors that are unknown at setup time. 

In this article, we suggest using existing process automation infrastructure based on alerting (PLC, SCADA, among others) and applying machine learning for dynamic process range based on changing conditions. We propose an unsupervised anomaly detection algorithm capable of online adaptation to change points and concept drifts, which adds to a recently developed body of research. The approach is evaluated on two case studies of microgrid sensors. To the author's knowledge, there are no studies to date concerned with providing adaptive operation constraints.

The main benefits of the proposed solution are that it:
\begin{itemize}
\item Keeps existing IT infrastructure, saving costs, and does not require operator retraining
\item Automates alerting thresholds setup for a high number of signals
\item Automates alerting for signals with no a priori knowledge of process limits
\item Assesses changing environmental conditions and device aging
\item Uses self-learning approach on streamed data
\end{itemize}
%Meanwhile, one of the problems with anomaly detection is that the anomaly may occur at any time. Prediction of the time, when an anomaly may occur is bound to an understanding of operational constraints. Some anomalies occur without any prior notice. However, patterns in data may yield important information about possible anomalies. Companies call for detectors that except the bare anomaly classification, offer information about real-time normal operation threshold for the measured value. Such information would allow for performance tracking, and process optimization, and open new prospects in the profitability and safety of IoT systems.

%In this study, we provide an unsupervised anomaly detection algorithm, capable of online adaptation to change points and concept drifts which adds to a recently developed body of research. Moreover, the goal of this study is to introduce the capability of providing a real-valued threshold for the input signal, which makes it possible to track the abnormality of the system and assess the state of operation. The method is evaluated on two case studies of microgrid sensors. To the author's knowledge, there are no studies to date concerned with providing adaptive normal operation constraints.